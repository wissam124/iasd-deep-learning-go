{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepLearningProject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wissam124/iasd-deep-learning-go/blob/master/DeepLearningProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-w-DMKhyuAv",
        "colab_type": "text"
      },
      "source": [
        "### Deep Learning Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cKxx5Sp04Lo",
        "colab_type": "text"
      },
      "source": [
        "This is the page for the Deep Learning Project of the master IASD. The goal is to train a network for playing the game of Go. In order to be fair about training ressources the number of parameters for the networks you submit must be lower than 1 000 000. The maximum number of students per team is two. The data used for training comes from Facebook ELF opengo Go program self played games. There are more than 98 000 000 different states in total in the training set. The input data is composed of 8 19x19 planes (color to play, ladders, current state on two planes, two previous states on four planes). The output targets are the policy (a vector of size 361 with 1.0 for the move played, 0.0 for the other moves), the value (1.0 if White won, 0.0 if Black won) and the state at the end of the game (two planes)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWUqojtke64C",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c31YVw4RxfU4",
        "colab_type": "code",
        "outputId": "ff2d2fce-49b0-4385-a491-31b17b311ee5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget https://www.lamsade.dauphine.fr/~cazenave/DeepLearningProject.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-30 14:35:35--  https://www.lamsade.dauphine.fr/~cazenave/DeepLearningProject.zip\n",
            "Resolving www.lamsade.dauphine.fr (www.lamsade.dauphine.fr)... 193.48.71.250\n",
            "Connecting to www.lamsade.dauphine.fr (www.lamsade.dauphine.fr)|193.48.71.250|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 211774472 (202M) [application/zip]\n",
            "Saving to: ‘DeepLearningProject.zip.2’\n",
            "\n",
            "DeepLearningProject 100%[===================>] 201.96M  11.8MB/s    in 19s     \n",
            "\n",
            "2019-12-30 14:35:56 (10.6 MB/s) - ‘DeepLearningProject.zip.2’ saved [211774472/211774472]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlFlboq_yzSk",
        "colab_type": "code",
        "outputId": "87527642-dc2c-40a9-b31c-42ed7c2c9fd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "!unzip -j DeepLearningProject.zip\n",
        "# Copy all files into root directory\n",
        "# !cp -r DeepLearningProject/* ."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  DeepLearningProject.zip\n",
            "replace Board.h? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: Board.h                 \n",
            "  inflating: Game.h                  \n",
            "  inflating: Rzone.h                 \n",
            "  inflating: compileMAC.sh           \n",
            "  inflating: compile.sh              \n",
            "  inflating: ls.sh                   \n",
            "  inflating: golois.cpp              \n",
            "  inflating: games.data              \n",
            "  inflating: golois.py               \n",
            "  inflating: end.npy                 \n",
            "  inflating: input_data.npy          \n",
            "  inflating: policy.npy              \n",
            "  inflating: value.npy               \n",
            "  inflating: README                  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V85wNDFey-V3",
        "colab_type": "code",
        "outputId": "fe81a4fe-fee0-4668-b09e-e0689c724207",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "!ls -all"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 2789096\n",
            "drwxr-xr-x 1 root root       4096 Dec 30 14:36 .\n",
            "drwxr-xr-x 1 root root       4096 Dec 30 13:39 ..\n",
            "-rw-r--r-- 1 root root     104265 Dec 10 12:17 Board.h\n",
            "-rwxrwxr-x 1 root root        224 Dec  5 21:19 compileMAC.sh\n",
            "-rwxr-xr-x 1 root root        156 Nov 23 12:23 compile.sh\n",
            "drwxr-xr-x 1 root root       4096 Dec 18 16:52 .config\n",
            "-rw-r--r-- 1 root root  211774472 Dec 10 13:51 DeepLearningProject.zip\n",
            "-rw-r--r-- 1 root root  211774472 Dec 10 13:51 DeepLearningProject.zip.1\n",
            "-rw-r--r-- 1 root root  211774472 Dec 10 13:51 DeepLearningProject.zip.2\n",
            "drwx------ 4 root root       4096 Dec 30 13:52 drive\n",
            "-rw-rw-r-- 1 root root  288800128 Dec  1 19:27 end.npy\n",
            "-rw-r--r-- 1 root root       7900 Dec 10 13:36 Game.h\n",
            "-rw-r--r-- 1 root root  631559220 Dec 10 12:28 games.data\n",
            "-rw-r--r-- 1 root root       3104 Dec  1 18:18 golois.cpp\n",
            "-rwxr-xr-x 1 root root     162520 Dec 30 13:51 golois.cpython-36m-x86_64-linux-gnu.so\n",
            "-rw-r--r-- 1 root root       1915 Dec  1 19:19 golois.py\n",
            "-rw-rw-r-- 1 root root 1155200128 Dec  1 19:27 input_data.npy\n",
            "-rwxr-xr-x 1 root root         52 Nov 24 12:05 ls.sh\n",
            "-rw-rw-r-- 1 root root  144400128 Dec  1 19:27 policy.npy\n",
            "-rw-rw-r-- 1 root root        757 Nov 29 00:23 README\n",
            "-rw-r--r-- 1 root root       6420 Dec  5 11:34 Rzone.h\n",
            "drwxr-xr-x 1 root root       4096 Dec 18 16:52 sample_data\n",
            "-rw-rw-r-- 1 root root     400128 Dec  1 19:27 value.npy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Md7ZkvCp-aK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r golois.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mq7X6QBxpfAn",
        "colab_type": "code",
        "outputId": "5c81c0b5-d0d9-4eea-f5b1-64483a79065c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip3 install pybind11"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pybind11 in /usr/local/lib/python3.6/dist-packages (2.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07WmuLhUpWzp",
        "colab_type": "code",
        "outputId": "a9379251-036f-49cb-d771-2ced4a8abd5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!./compile.sh"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In file included from \u001b[01m\u001b[Kgolois.cpp:17:0\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[KBoard.h:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kbool Board::isCapturedLadder(int, int, Rzone*)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[KBoard.h:1742:8:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kn1\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "    int \u001b[01;35m\u001b[Kn1\u001b[m\u001b[K = nbLiberties (inter, liberties1, stones1, 3);\n",
            "        \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[KBoard.h:1763:12:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kn1\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "        int \u001b[01;35m\u001b[Kn1\u001b[m\u001b[K = nbLiberties (inter, liberties1, stones1, 3);\n",
            "            \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[KBoard.h:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kvoid Board::computeLadders(int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[KBoard.h:1792:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kother\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     int \u001b[01;35m\u001b[Kother\u001b[m\u001b[K = opponent (color);\n",
            "         \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[KBoard.h:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kvoid Board::computeAllLadders(int, bool)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[KBoard.h:2038:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kn1\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     int \u001b[01;35m\u001b[Kn1\u001b[m\u001b[K = nbLiberties (i, liberties1, stones1);\n",
            "         \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[KBoard.h:2074:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kn1\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "   int \u001b[01;35m\u001b[Kn1\u001b[m\u001b[K = nbLiberties (i, liberties1, stones1);\n",
            "       \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[KBoard.h:2107:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kn1\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     int \u001b[01;35m\u001b[Kn1\u001b[m\u001b[K = nbLiberties (i, liberties1, stones1);\n",
            "         \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[KBoard.h:2144:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kn1\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "   int \u001b[01;35m\u001b[Kn1\u001b[m\u001b[K = nbLiberties (i, liberties1, stones1);\n",
            "       \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[KBoard.h:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kvoid Board::printLadders(FILE*, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[KBoard.h:2216:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kother\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     int \u001b[01;35m\u001b[Kother\u001b[m\u001b[K = opponent (color);\n",
            "         \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[KBoard.h:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kbool Board::sameString(int, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[KBoard.h:2265:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kn\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     int \u001b[01;35m\u001b[Kn\u001b[m\u001b[K = nbLiberties (inter, liberties, stones);\n",
            "         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[KBoard.h:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kbool Board::atariLent(int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[KBoard.h:2584:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Ksuggest explicit braces to avoid ambiguous ‘\u001b[01m\u001b[Kelse\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wdangling-else\u001b[m\u001b[K]\n",
            "       if \u001b[01;35m\u001b[K(\u001b[m\u001b[Kboard [pierre - 1] == Empty)\n",
            "          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[KBoard.h:2587:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Ksuggest explicit braces to avoid ambiguous ‘\u001b[01m\u001b[Kelse\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wdangling-else\u001b[m\u001b[K]\n",
            "       if \u001b[01;35m\u001b[K(\u001b[m\u001b[Kboard [pierre + 1] == Empty)\n",
            "          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[KBoard.h:2590:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Ksuggest explicit braces to avoid ambiguous ‘\u001b[01m\u001b[Kelse\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wdangling-else\u001b[m\u001b[K]\n",
            "       if \u001b[01;35m\u001b[K(\u001b[m\u001b[Kboard [pierre - dxBoard] == Empty)\n",
            "          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[KBoard.h:2593:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Ksuggest explicit braces to avoid ambiguous ‘\u001b[01m\u001b[Kelse\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wdangling-else\u001b[m\u001b[K]\n",
            "       if \u001b[01;35m\u001b[K(\u001b[m\u001b[Kboard [pierre + dxBoard] == Empty)\n",
            "          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[KBoard.h:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kint Board::playAtariIfCapture(int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[KBoard.h:2972:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kother\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     int \u001b[01;35m\u001b[Kother\u001b[m\u001b[K = opponent (couleur);\n",
            "         \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[KBoard.h:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kint Board::captureAdjacentAtari(int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[KBoard.h:3028:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kother\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     int \u001b[01;35m\u001b[Kother\u001b[m\u001b[K = opponent (couleur);\n",
            "         \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[KBoard.h:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kint Board::captureAtari(int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[KBoard.h:3064:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kother\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     int \u001b[01;35m\u001b[Kother\u001b[m\u001b[K = opponent (couleur);\n",
            "         \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[KBoard.h:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kint Board::avoidCaptureAtari(int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[KBoard.h:3087:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kother\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     int \u001b[01;35m\u001b[Kother\u001b[m\u001b[K = opponent (couleur);\n",
            "         \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[KBoard.h:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kint Board::playout(int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[KBoard.h:3223:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kvariable ‘\u001b[01m\u001b[Km\u001b[m\u001b[K’ set but not used [\u001b[01;35m\u001b[K-Wunused-but-set-variable\u001b[m\u001b[K]\n",
            "  Move \u001b[01;35m\u001b[Km\u001b[m\u001b[K;\n",
            "       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[KBoard.h:3244:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Ks\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "    char \u001b[01;35m\u001b[Ks\u001b[m\u001b[K [256];\n",
            "         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[KBoard.h:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kint Board::loadSGF(FILE*)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[KBoard.h:3623:8:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kres\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "    int \u001b[01;35m\u001b[Kres\u001b[m\u001b[K = sscanf (InsideBracket, \"%d\", &sz);\n",
            "        \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[KBoard.h:3630:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kres\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "      int \u001b[01;35m\u001b[Kres\u001b[m\u001b[K = sscanf (InsideBracket, \"%d: %f\", &i, &v);\n",
            "          \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[Kgolois.cpp:18:0\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[KGame.h:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid loadGamesData(char*)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[KGame.h:128:69:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%d\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kint*\u001b[m\u001b[K’, but argument 3 has type ‘\u001b[01m\u001b[Kshort int*\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "  fscanf (fp, \"%d %d\", \u001b[32m\u001b[K&proGame [g] [i].inter\u001b[m\u001b[K, &proGame [g] [i].color\u001b[01;35m\u001b[K)\u001b[m\u001b[K;\n",
            "                       \u001b[32m\u001b[K~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[KGame.h:128:69:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%d\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kint*\u001b[m\u001b[K’, but argument 4 has type ‘\u001b[01m\u001b[Kchar*\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "\u001b[01m\u001b[KGame.h:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid loadGamesDataVal(char*)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[KGame.h:185:95:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%d\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kint*\u001b[m\u001b[K’, but argument 3 has type ‘\u001b[01m\u001b[Kshort int*\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "  fscanf (fp, \"%d %f %d \", \u001b[32m\u001b[K&proGame [g] [i].inter\u001b[m\u001b[K, &proGame [g] [i].val, &proGame [g] [i].color\u001b[01;35m\u001b[K)\u001b[m\u001b[K;\n",
            "                           \u001b[32m\u001b[K~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K                                              \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[KGame.h:185:95:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%d\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kint*\u001b[m\u001b[K’, but argument 5 has type ‘\u001b[01m\u001b[Kchar*\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "\u001b[01m\u001b[Kgolois.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kgolois.cpp:32:31:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C++ forbids converting a string constant to ‘\u001b[01m\u001b[Kchar*\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wwrite-strings\u001b[m\u001b[K]\n",
            "    loadGamesData (\"games.data\"\u001b[01;35m\u001b[K)\u001b[m\u001b[K;\n",
            "                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kgolois.cpp:40:101:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%d\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’, but argument 3 has type ‘\u001b[01m\u001b[Kpybind11::ssize_t {aka long int}\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "  fprintf (stderr, \"r.shape = (%d, %d, %d, %d)\\n\", \u001b[32m\u001b[Kr.shape (0)\u001b[m\u001b[K, r.shape (1), r.shape (2), r.shape (3)\u001b[01;35m\u001b[K)\u001b[m\u001b[K;\n",
            "                                                   \u001b[32m\u001b[K~~~~~~~~~~~\u001b[m\u001b[K                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kgolois.cpp:40:101:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%d\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’, but argument 4 has type ‘\u001b[01m\u001b[Kpybind11::ssize_t {aka long int}\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "\u001b[01m\u001b[Kgolois.cpp:40:101:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%d\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’, but argument 5 has type ‘\u001b[01m\u001b[Kpybind11::ssize_t {aka long int}\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "\u001b[01m\u001b[Kgolois.cpp:40:101:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%d\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’, but argument 6 has type ‘\u001b[01m\u001b[Kpybind11::ssize_t {aka long int}\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "In file included from \u001b[01m\u001b[Kgolois.cpp:18:0\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[KGame.h:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid loadGamesData(char*)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[KGame.h:121:12:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kint fscanf(FILE*, const char*, ...)\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
            "     \u001b[01;35m\u001b[Kfscanf (fp, \"%d\", &nbGames)\u001b[m\u001b[K;\n",
            "     \u001b[01;35m\u001b[K~~~~~~~^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[KGame.h:123:14:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kint fscanf(FILE*, const char*, ...)\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
            "       \u001b[01;35m\u001b[Kfscanf (fp, \"%s \", s)\u001b[m\u001b[K;\n",
            "       \u001b[01;35m\u001b[K~~~~~~~^~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[KGame.h:125:14:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kint fscanf(FILE*, const char*, ...)\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
            "       \u001b[01;35m\u001b[Kfscanf (fp, \"%d\", &nbMovesSGFGame [g])\u001b[m\u001b[K;\n",
            "       \u001b[01;35m\u001b[K~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[KGame.h:128:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kint fscanf(FILE*, const char*, ...)\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
            "  \u001b[01;35m\u001b[Kfscanf (fp, \"%d %d\", &proGame [g] [i].inter, &proGame [g] [i].color)\u001b[m\u001b[K;\n",
            "  \u001b[01;35m\u001b[K~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[KGame.h:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid loadGamesDataVal(char*)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[KGame.h:178:12:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kint fscanf(FILE*, const char*, ...)\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
            "     \u001b[01;35m\u001b[Kfscanf (fp, \"%d\", &nbGames)\u001b[m\u001b[K;\n",
            "     \u001b[01;35m\u001b[K~~~~~~~^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[KGame.h:180:14:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kint fscanf(FILE*, const char*, ...)\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
            "       \u001b[01;35m\u001b[Kfscanf (fp, \"%s \", s)\u001b[m\u001b[K;\n",
            "       \u001b[01;35m\u001b[K~~~~~~~^~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[KGame.h:182:14:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kint fscanf(FILE*, const char*, ...)\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
            "       \u001b[01;35m\u001b[Kfscanf (fp, \"%d\", &nbMovesSGFGame [g])\u001b[m\u001b[K;\n",
            "       \u001b[01;35m\u001b[K~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[KGame.h:185:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kint fscanf(FILE*, const char*, ...)\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
            "  \u001b[01;35m\u001b[Kfscanf (fp, \"%d %f %d \", &proGame [g] [i].inter, &proGame [g] [i].val, &proGame [g] [i].color)\u001b[m\u001b[K;\n",
            "  \u001b[01;35m\u001b[K~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFNUVbq9pLz7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Input, Dense, Conv2D, Flatten, BatchNormalization, Activation, LeakyReLU, add, SpatialDropout2D, ReLU, Softmax, MaxPool2D, Dropout\n",
        "from tensorflow.keras.optimizers import SGD, Adam, Adadelta\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import golois\n",
        "\n",
        "class GoModel():\n",
        "    def __init__(self, regParam, learningRate, inputDim, outputDim):\n",
        "        self.regParam = regParam\n",
        "        self.learningRate = learningRate\n",
        "        self.inputDim = inputDim\n",
        "        self.outputDim = outputDim\n",
        "\n",
        "    def predict(self, x):\n",
        "        return self.model.predict(x)\n",
        "\n",
        "    def fit(self, X, y, epochs, verbose, validation_split, batch_size):\n",
        "\n",
        "        ValCheckpoint = ModelCheckpoint('best_val_loss.h5',\n",
        "                                monitor='val_loss',\n",
        "                                verbose=1,\n",
        "                                save_best_only=True,\n",
        "                                mode='auto',\n",
        "                                period=1)\n",
        "\n",
        "        csv_logger = CSVLogger('training.log', separator=',', append=False)\n",
        "\n",
        "        return self.model.fit(X,\n",
        "                              y,\n",
        "                              epochs=epochs,\n",
        "                              verbose=verbose,\n",
        "                              validation_split=validation_split,\n",
        "                              batch_size=batch_size,\n",
        "                              callbacks=[ValCheckpoint, csv_logger])\n",
        "\n",
        "    def save_model(self):\n",
        "        self.model.save('./model_' + \n",
        "                        str(self.regParam) + 'reg' +\n",
        "                        '.h5')\n",
        "\n",
        "    def summary(self):\n",
        "        return self.model.summary()\n",
        "\n",
        "    def plot_model(self):\n",
        "        plot_model(self.model)\n",
        "\n",
        "    def display_layers():\n",
        "        pass\n",
        "\n",
        "\n",
        "class NeuralNet(GoModel):\n",
        "    def __init__(self, regParam, learningRate, inputDim, outputDim,\n",
        "                 hiddenLayers, momentum):\n",
        "        GoModel.__init__(self, regParam, learningRate, inputDim, outputDim)\n",
        "        self.hidden_layers = hiddenLayers\n",
        "        self.momentum = momentum\n",
        "        self.num_layers = len(hiddenLayers)\n",
        "        self.model = self.buildModel()\n",
        "\n",
        "    def convLayer(self, x, numFilters, kernelSize):\n",
        "\n",
        "        x = Conv2D(filters=numFilters,\n",
        "                   kernel_size=kernelSize,\n",
        "                   data_format='channels_last',\n",
        "                   padding='same',\n",
        "                   use_bias=False,\n",
        "                   activation='linear',\n",
        "                   kernel_regularizer=regularizers.l2(self.regParam))(x)\n",
        "\n",
        "        x = SpatialDropout2D(rate=0.5,\n",
        "                             data_format='channels_last')(x)\n",
        "\n",
        "        x = BatchNormalization(axis=-1)(x)\n",
        "\n",
        "        x = LeakyReLU(alpha=0.3)(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def residualLayer(self, inputLayer, numFilters, kernelSize):\n",
        "\n",
        "        x = self.convLayer(inputLayer, numFilters, kernelSize)\n",
        "\n",
        "        x = Conv2D(filters=numFilters,\n",
        "                   kernel_size=kernelSize,\n",
        "                   data_format='channels_last',\n",
        "                   padding='same',\n",
        "                   use_bias=False,\n",
        "                   activation='linear',\n",
        "                   kernel_regularizer=regularizers.l2(self.regParam))(x)\n",
        "\n",
        "        x = SpatialDropout2D(rate=0.5,\n",
        "                             data_format='channels_last')(x)\n",
        "\n",
        "        x = BatchNormalization(axis=-1)(x)\n",
        "\n",
        "        x = add([inputLayer, x])\n",
        "\n",
        "        x = LeakyReLU(alpha=0.3)(x)\n",
        "\n",
        "        return (x)\n",
        "\n",
        "    def value_head(self, x):\n",
        "\n",
        "        x = Conv2D(filters=1,\n",
        "                   kernel_size=(1, 1),\n",
        "                   data_format='channels_last',\n",
        "                   padding='same',\n",
        "                   use_bias=False,\n",
        "                   activation='linear',\n",
        "                   kernel_regularizer=regularizers.l2(self.regParam))(x)\n",
        "        \n",
        "        x = BatchNormalization(axis=-1)(x)\n",
        "\n",
        "        x = LeakyReLU(alpha=0.3)(x)\n",
        "\n",
        "        x = Flatten()(x)\n",
        "\n",
        "        x = Dense(40,\n",
        "                  use_bias=False,\n",
        "                  activation='linear',\n",
        "                  kernel_regularizer=regularizers.l2(self.regParam))(x)\n",
        "\n",
        "        x = LeakyReLU(alpha=0.3)(x)\n",
        "\n",
        "        # x = Dropout(0.2)(x)\n",
        "\n",
        "        x = Dense(1,\n",
        "                  use_bias=False,\n",
        "                  activation='sigmoid',\n",
        "                  kernel_regularizer=regularizers.l2(self.regParam),\n",
        "                  name='value')(x)\n",
        "\n",
        "        return (x)\n",
        "\n",
        "    def policy_head(self, x):\n",
        "\n",
        "        x = Conv2D(filters=2,\n",
        "                   kernel_size=(1, 1),\n",
        "                   data_format='channels_last',\n",
        "                   padding='same',\n",
        "                   use_bias=False,\n",
        "                   activation='linear',\n",
        "                   kernel_regularizer=regularizers.l2(self.regParam))(x)\n",
        "\n",
        "        x = BatchNormalization(axis=-1)(x)\n",
        "\n",
        "        x = LeakyReLU(alpha=0.3)(x)\n",
        "\n",
        "        x = Flatten()(x)\n",
        "\n",
        "        x = Dense(self.outputDim, activation='softmax', name='policy')(x)\n",
        "\n",
        "        return (x)\n",
        "\n",
        "    def buildModel(self):\n",
        "\n",
        "        mainInput = Input(shape=self.inputDim, name='board')\n",
        "\n",
        "        x = self.convLayer(mainInput, self.hidden_layers[0]['numFilters'],\n",
        "                           self.hidden_layers[0]['kernelSize'])\n",
        "\n",
        "        if len(self.hidden_layers) > 1:\n",
        "            for h in self.hidden_layers[1:]:\n",
        "                x = self.residualLayer(x, h['numFilters'], h['kernelSize'])\n",
        "\n",
        "        value_head = self.value_head(x)\n",
        "        policy_head = self.policy_head(x)\n",
        "\n",
        "        model = Model(inputs=[mainInput], outputs=[policy_head, value_head])\n",
        "\n",
        "        # model.compile(optimizer=SGD(lr=self.learningRate, momentum=self.momentum),\n",
        "        #               loss={\n",
        "        #                   'value': 'mse',\n",
        "        #                   'policy': 'categorical_crossentropy'\n",
        "        #               },\n",
        "        #               loss_weights={\n",
        "        #                   'value': 100,\n",
        "        #                   'policy':1\n",
        "        #               },\n",
        "        #               metrics=['accuracy'])\n",
        "\n",
        "        model.compile(optimizer=SGD(lr=self.learningRate, momentum=self.momentum),\n",
        "                      loss={\n",
        "                          'value': 'mse',\n",
        "                          'policy': 'categorical_crossentropy'\n",
        "                      },\n",
        "                      loss_weights={\n",
        "                          'value': 1,\n",
        "                          'policy':1\n",
        "                      },\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sETUz717cPrQ",
        "colab_type": "code",
        "outputId": "a42214ea-c7c3-4821-a9f1-406f17dd926e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.15.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXtxUGZARTZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generateData(N=10000, dynamicBatch=False):\n",
        "    planes = 8\n",
        "    moves = 361\n",
        "    dynamicBatch = True  # Pour tester réseau en générant des parties avec la librairie Golois\n",
        "    if dynamicBatch:\n",
        "        input_data = np.random.randint(2, size=(N, 19, 19, planes))\n",
        "        input_data = input_data.astype('float32')\n",
        "\n",
        "        policy = np.random.randint(moves, size=(N, ))\n",
        "        policy = keras.utils.to_categorical(policy)\n",
        "\n",
        "        value = np.random.randint(2, size=(N, ))\n",
        "        value = value.astype('float32')\n",
        "\n",
        "        end = np.random.randint(2, size=(N, 19, 19, 2))\n",
        "        end = end.astype('float32')\n",
        "\n",
        "        golois.getBatch(input_data, policy, value, end)\n",
        "    else:\n",
        "        input_data = np.load('./input_data.npy')\n",
        "        policy = np.load('./policy.npy')\n",
        "        value = np.load('./value.npy')\n",
        "        # end = np.load('./end.npy')\n",
        "    \n",
        "    return input_data, policy, value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTtNpFoHqW1Q",
        "colab_type": "code",
        "outputId": "b7db45fb-7a96-4cc5-eacb-02e04c06e06d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "input_data, policy, value = generateData(300000, True)\n",
        "input_data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300000, 19, 19, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKaUIHsKSo8h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parameters\n",
        "BATCH_SIZE = 256\n",
        "EPOCHS = 30\n",
        "REG_CONST = 0.001\n",
        "LEARNING_RATE = 0.001\n",
        "MOMENTUM = 0.9\n",
        "\n",
        "HIDDEN_CNN_LAYERS = [{\n",
        "    'numFilters': 64,\n",
        "    'kernelSize': (7, 7)\n",
        "}, {\n",
        "    'numFilters': 64,\n",
        "    'kernelSize': (5, 5)\n",
        "}, {\n",
        "    'numFilters': 64,\n",
        "    'kernelSize': (5, 5)\n",
        "}, {\n",
        "    'numFilters': 64,\n",
        "    'kernelSize': (3, 3)\n",
        "}, {\n",
        "    'numFilters': 64,\n",
        "    'kernelSize': (3, 3)\n",
        "}, {\n",
        "    'numFilters': 64,\n",
        "    'kernelSize': (3, 3)\n",
        "}]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyJxpxZkfbF3",
        "colab_type": "code",
        "outputId": "275cee0b-8427-41dd-91e4-3c647e5cfa74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "nHiddenLayers = len(HIDDEN_CNN_LAYERS)\n",
        "print(len(HIDDEN_CNN_LAYERS))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XONlcqVRS1Hn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create Go Neural Network\n",
        "moves = 361\n",
        "GoNeuralNet = NeuralNet(REG_CONST, LEARNING_RATE,\n",
        "                        (19, 19, 8), moves, HIDDEN_CNN_LAYERS,\n",
        "                        MOMENTUM)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zwvx-DeW-HQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Display summary of neural network\n",
        "GoNeuralNet.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pchGZL2YsZY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Plot model\n",
        "# GoNeuralNet.plot_model()\n",
        "# from IPython.display import Image\n",
        "# Image('model.png');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TylBKS1XAlA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GoNeuralNet.fit(input_data, {\n",
        "    'policy': policy,\n",
        "    'value': value\n",
        "},\n",
        "                epochs=25,\n",
        "                verbose=1,\n",
        "                validation_split=0.1,\n",
        "                batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbRfz8FinAE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "plt.style.use('seaborn')\n",
        "df = pd.read_csv('./training.log')\n",
        "epochs = df['epoch']\n",
        "plt.clf()\n",
        "f, ax = plt.subplots(2, 3, figsize=(20,10))\n",
        "ax[0][0].plot(epochs, df['loss'])\n",
        "ax[0][0].plot(epochs, df['val_loss'])\n",
        "ax[0][0].legend(['loss', 'val_los'])\n",
        "ax[0][0].set_title('Total loss')\n",
        "ax[0][1].plot(epochs, df['policy_loss'])\n",
        "ax[0][1].plot(epochs, df['val_policy_loss'])\n",
        "ax[0][1].legend(['policy_loss', 'val_policy_loss'])\n",
        "ax[0][1].set_title('Policy loss')\n",
        "ax[0][2].plot(epochs, df['value_loss'])\n",
        "ax[0][2].plot(epochs, df['val_value_loss'])\n",
        "ax[0][2].legend(['value_loss', 'val_value_loss'])\n",
        "ax[0][2].set_title('Value loss')\n",
        "ax[1][1].plot(epochs, df['policy_acc'])\n",
        "ax[1][1].plot(epochs, df['val_policy_acc'])\n",
        "ax[1][1].legend(['policy_acc', 'val_policy_acc'])\n",
        "ax[1][1].set_title('Policy acc')\n",
        "ax[1][2].plot(epochs, df['value_acc'])\n",
        "ax[1][2].plot(epochs, df['val_value_acc'])\n",
        "ax[1][2].legend(['value_acc', 'val_value_acc'])\n",
        "ax[1][2].set_title('Value accuarcy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvRvtcAPKlgA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save weights and training log\n",
        "from google.colab import files\n",
        "# files.download('best_train_loss.h5')\n",
        "files.download('best_val_loss.h5')\n",
        "# files.download('training.log')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cj-cK84szvf9",
        "colab_type": "code",
        "outputId": "9185f905-eea8-45d4-e376-3555b481f3dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# Mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYE8eirt1ltF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load pre-trained model from Drive\n",
        "from tensorflow.keras.models import load_model\n",
        "new_model = load_model('./drive/My Drive/DLGO/Dupin_Rynkiewicz_30_64.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h27BkvVQ22A2",
        "colab_type": "code",
        "outputId": "f4b53550-de78-4faf-dcb5-fe92d18a59e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Baseline original model from first submission\n",
        "# new_model.summary() #  \n",
        "new_model.evaluate(input_data, {'policy': policy, 'value': value})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "300000/300000 [==============================] - 62s 208us/sample - loss: 3.3899 - policy_loss: 2.9810 - value_loss: 0.2180 - policy_accuracy: 0.3094 - value_accuracy: 0.6433\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3.3898697851308186, 2.9809532, 0.21800779, 0.30939, 0.64327]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcPpl0A3jldX",
        "colab_type": "text"
      },
      "source": [
        "# Iterative Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWJb2987e9jQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load pre-trained model from iterative training\n",
        "from tensorflow.keras.models import load_model\n",
        "new_model = load_model('best_val_loss_iter_train.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2P-cRMivxJfX",
        "colab_type": "code",
        "outputId": "38f3e003-947a-43a0-8e9e-966b76a2631f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Set layers to be trained\n",
        "my_layers = new_model.layers\n",
        "for layer in new_model.layers:\n",
        "    layer.trainable = True\n",
        "print(new_model.layers[34].trainable)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpT2qPxOhswZ",
        "colab_type": "code",
        "outputId": "7c38177e-b72c-4442-c2b4-87d4eea08a64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "new_model.evaluate(input_data, {'policy': policy, 'value': value})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "300000/300000 [==============================] - 64s 214us/sample - loss: 3.2713 - policy_loss: 2.9634 - value_loss: 0.2168 - policy_acc: 0.3105 - value_acc: 0.6441\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3.2712620943196615, 2.9633567, 0.21683921, 0.31052667, 0.64411336]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfQMsz0DnZJp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Recompile model before retraining\n",
        "# new_model.compile(optimizer=SGD(lr=0.001, momentum=0.9),\n",
        "#                       loss={\n",
        "#                           'value': 'mse',\n",
        "#                           'policy': 'categorical_crossentropy'\n",
        "#                       },\n",
        "#                       loss_weights={\n",
        "#                           'value': 1,\n",
        "#                           'policy': 1\n",
        "#                       },\n",
        "#                       metrics=['accuracy'])\n",
        "\n",
        "# Recompile model before retraining\n",
        "new_model.compile(optimizer=SGD(learning_rate=0.01),\n",
        "                      loss={\n",
        "                          'value': 'mse',\n",
        "                          'policy': 'categorical_crossentropy'\n",
        "                      },\n",
        "                      loss_weights={\n",
        "                          'value': 1,\n",
        "                          'policy': 1\n",
        "                      },\n",
        "                      metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k214Bt5UONzA",
        "colab_type": "code",
        "outputId": "04eff42d-4817-45cd-bf3c-441b7f1e4af6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "ValCheckpoint = ModelCheckpoint('best_val_loss_iter_train.h5',\n",
        "                        monitor='val_loss',\n",
        "                        verbose=1,\n",
        "                        save_best_only=True,\n",
        "                        mode='auto',\n",
        "                        period=1)\n",
        "\n",
        "\n",
        "# es = EarlyStopping(monitor='val_loss', \n",
        "#                    patience=7,\n",
        "#                    verbose=1,\n",
        "#                    mode='min', \n",
        "#                    restore_best_weights=True)\n",
        "\n",
        "\n",
        "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
        "#                               factor=0.5,\n",
        "#                               patience=3,\n",
        "#                               verbose=1,\n",
        "#                               mode='min',  \n",
        "#                               min_lr=0.00001)\n",
        "\n",
        "for i in range(20):\n",
        "    input_data_iter, policy_iter, value_iter = generateData(N=100000)\n",
        "\n",
        "\n",
        "    new_model.fit(input_data_iter, \n",
        "                {'policy': policy_iter,\n",
        "                'value': value_iter},\n",
        "                epochs=3,\n",
        "                verbose=1,\n",
        "                validation_split=0.1,\n",
        "                batch_size=256,\n",
        "                callbacks=[ValCheckpoint])\n",
        "    \n",
        "    del input_data_iter\n",
        "    del policy_iter\n",
        "    del value_iter\n",
        "    print('end of iteration :', i)\n",
        "    # new_model.evaluate(input_data, {'policy': policy, 'value': value})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n",
            "Train on 90000 samples, validate on 10000 samples\n",
            "Epoch 1/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5968 - policy_loss: 3.2888 - value_loss: 0.2205 - policy_acc: 0.2645 - value_acc: 0.6392\n",
            "Epoch 00001: val_loss improved from inf to 3.41184, saving model to best_val_loss_iter_train.h5\n",
            "90000/90000 [==============================] - 55s 611us/sample - loss: 3.5966 - policy_loss: 3.2883 - value_loss: 0.2205 - policy_acc: 0.2646 - value_acc: 0.6392 - val_loss: 3.4118 - val_policy_loss: 3.0883 - val_value_loss: 0.2174 - val_policy_acc: 0.2818 - val_value_acc: 0.6402\n",
            "Epoch 2/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5809 - policy_loss: 3.2731 - value_loss: 0.2204 - policy_acc: 0.2666 - value_acc: 0.6385\n",
            "Epoch 00002: val_loss improved from 3.41184 to 3.40709, saving model to best_val_loss_iter_train.h5\n",
            "90000/90000 [==============================] - 27s 295us/sample - loss: 3.5812 - policy_loss: 3.2737 - value_loss: 0.2204 - policy_acc: 0.2666 - value_acc: 0.6386 - val_loss: 3.4071 - val_policy_loss: 3.1162 - val_value_loss: 0.2156 - val_policy_acc: 0.2826 - val_value_acc: 0.6443\n",
            "Epoch 3/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5657 - policy_loss: 3.2577 - value_loss: 0.2208 - policy_acc: 0.2673 - value_acc: 0.6385\n",
            "Epoch 00003: val_loss did not improve from 3.40709\n",
            "90000/90000 [==============================] - 26s 293us/sample - loss: 3.5659 - policy_loss: 3.2581 - value_loss: 0.2208 - policy_acc: 0.2673 - value_acc: 0.6386 - val_loss: 3.4435 - val_policy_loss: 3.1367 - val_value_loss: 0.2172 - val_policy_acc: 0.2798 - val_value_acc: 0.6472\n",
            "end of iteration : 0\n",
            "Train on 90000 samples, validate on 10000 samples\n",
            "Epoch 1/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5984 - policy_loss: 3.2895 - value_loss: 0.2217 - policy_acc: 0.2649 - value_acc: 0.6348\n",
            "Epoch 00001: val_loss did not improve from 3.40709\n",
            "90000/90000 [==============================] - 26s 294us/sample - loss: 3.5988 - policy_loss: 3.2902 - value_loss: 0.2218 - policy_acc: 0.2648 - value_acc: 0.6347 - val_loss: 3.4313 - val_policy_loss: 3.1298 - val_value_loss: 0.2142 - val_policy_acc: 0.2836 - val_value_acc: 0.6561\n",
            "Epoch 2/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5756 - policy_loss: 3.2671 - value_loss: 0.2215 - policy_acc: 0.2671 - value_acc: 0.6359\n",
            "Epoch 00002: val_loss did not improve from 3.40709\n",
            "90000/90000 [==============================] - 26s 293us/sample - loss: 3.5752 - policy_loss: 3.2665 - value_loss: 0.2215 - policy_acc: 0.2672 - value_acc: 0.6360 - val_loss: 3.4615 - val_policy_loss: 3.1838 - val_value_loss: 0.2164 - val_policy_acc: 0.2841 - val_value_acc: 0.6548\n",
            "Epoch 3/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5629 - policy_loss: 3.2548 - value_loss: 0.2211 - policy_acc: 0.2695 - value_acc: 0.6355\n",
            "Epoch 00003: val_loss did not improve from 3.40709\n",
            "90000/90000 [==============================] - 26s 293us/sample - loss: 3.5629 - policy_loss: 3.2548 - value_loss: 0.2211 - policy_acc: 0.2695 - value_acc: 0.6355 - val_loss: 3.4843 - val_policy_loss: 3.1800 - val_value_loss: 0.2151 - val_policy_acc: 0.2778 - val_value_acc: 0.6552\n",
            "end of iteration : 1\n",
            "Train on 90000 samples, validate on 10000 samples\n",
            "Epoch 1/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5930 - policy_loss: 3.2851 - value_loss: 0.2211 - policy_acc: 0.2650 - value_acc: 0.6369\n",
            "Epoch 00001: val_loss did not improve from 3.40709\n",
            "90000/90000 [==============================] - 26s 293us/sample - loss: 3.5927 - policy_loss: 3.2846 - value_loss: 0.2211 - policy_acc: 0.2651 - value_acc: 0.6370 - val_loss: 3.4089 - val_policy_loss: 3.0940 - val_value_loss: 0.2162 - val_policy_acc: 0.2873 - val_value_acc: 0.6416\n",
            "Epoch 2/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5715 - policy_loss: 3.2640 - value_loss: 0.2209 - policy_acc: 0.2662 - value_acc: 0.6379\n",
            "Epoch 00002: val_loss did not improve from 3.40709\n",
            "90000/90000 [==============================] - 26s 294us/sample - loss: 3.5713 - policy_loss: 3.2636 - value_loss: 0.2208 - policy_acc: 0.2663 - value_acc: 0.6379 - val_loss: 3.4373 - val_policy_loss: 3.1395 - val_value_loss: 0.2178 - val_policy_acc: 0.2869 - val_value_acc: 0.6418\n",
            "Epoch 3/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5582 - policy_loss: 3.2508 - value_loss: 0.2208 - policy_acc: 0.2701 - value_acc: 0.6379\n",
            "Epoch 00003: val_loss did not improve from 3.40709\n",
            "90000/90000 [==============================] - 26s 293us/sample - loss: 3.5583 - policy_loss: 3.2509 - value_loss: 0.2209 - policy_acc: 0.2701 - value_acc: 0.6378 - val_loss: 3.4158 - val_policy_loss: 3.1068 - val_value_loss: 0.2179 - val_policy_acc: 0.2834 - val_value_acc: 0.6409\n",
            "end of iteration : 2\n",
            "Train on 90000 samples, validate on 10000 samples\n",
            "Epoch 1/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.6039 - policy_loss: 3.2967 - value_loss: 0.2208 - policy_acc: 0.2615 - value_acc: 0.6381\n",
            "Epoch 00001: val_loss did not improve from 3.40709\n",
            "90000/90000 [==============================] - 27s 294us/sample - loss: 3.6039 - policy_loss: 3.2968 - value_loss: 0.2207 - policy_acc: 0.2614 - value_acc: 0.6382 - val_loss: 3.4713 - val_policy_loss: 3.1651 - val_value_loss: 0.2179 - val_policy_acc: 0.2817 - val_value_acc: 0.6373\n",
            "Epoch 2/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5819 - policy_loss: 3.2747 - value_loss: 0.2209 - policy_acc: 0.2642 - value_acc: 0.6377\n",
            "Epoch 00002: val_loss did not improve from 3.40709\n",
            "90000/90000 [==============================] - 26s 294us/sample - loss: 3.5820 - policy_loss: 3.2747 - value_loss: 0.2209 - policy_acc: 0.2642 - value_acc: 0.6377 - val_loss: 3.4497 - val_policy_loss: 3.1410 - val_value_loss: 0.2175 - val_policy_acc: 0.2828 - val_value_acc: 0.6410\n",
            "Epoch 3/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5658 - policy_loss: 3.2587 - value_loss: 0.2208 - policy_acc: 0.2674 - value_acc: 0.6379\n",
            "Epoch 00003: val_loss did not improve from 3.40709\n",
            "90000/90000 [==============================] - 27s 295us/sample - loss: 3.5653 - policy_loss: 3.2577 - value_loss: 0.2209 - policy_acc: 0.2675 - value_acc: 0.6379 - val_loss: 3.4917 - val_policy_loss: 3.1705 - val_value_loss: 0.2190 - val_policy_acc: 0.2768 - val_value_acc: 0.6351\n",
            "end of iteration : 3\n",
            "Train on 90000 samples, validate on 10000 samples\n",
            "Epoch 1/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5799 - policy_loss: 3.2730 - value_loss: 0.2207 - policy_acc: 0.2655 - value_acc: 0.6390\n",
            "Epoch 00001: val_loss did not improve from 3.40709\n",
            "90000/90000 [==============================] - 26s 294us/sample - loss: 3.5799 - policy_loss: 3.2729 - value_loss: 0.2207 - policy_acc: 0.2655 - value_acc: 0.6389 - val_loss: 3.4363 - val_policy_loss: 3.1282 - val_value_loss: 0.2189 - val_policy_acc: 0.2847 - val_value_acc: 0.6373\n",
            "Epoch 2/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5589 - policy_loss: 3.2519 - value_loss: 0.2209 - policy_acc: 0.2680 - value_acc: 0.6387\n",
            "Epoch 00002: val_loss did not improve from 3.40709\n",
            "90000/90000 [==============================] - 26s 294us/sample - loss: 3.5590 - policy_loss: 3.2521 - value_loss: 0.2208 - policy_acc: 0.2679 - value_acc: 0.6388 - val_loss: 3.4239 - val_policy_loss: 3.1562 - val_value_loss: 0.2186 - val_policy_acc: 0.2881 - val_value_acc: 0.6360\n",
            "Epoch 3/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5416 - policy_loss: 3.2347 - value_loss: 0.2208 - policy_acc: 0.2713 - value_acc: 0.6387\n",
            "Epoch 00003: val_loss did not improve from 3.40709\n",
            "90000/90000 [==============================] - 26s 293us/sample - loss: 3.5419 - policy_loss: 3.2353 - value_loss: 0.2208 - policy_acc: 0.2713 - value_acc: 0.6387 - val_loss: 3.4178 - val_policy_loss: 3.0992 - val_value_loss: 0.2186 - val_policy_acc: 0.2827 - val_value_acc: 0.6379\n",
            "end of iteration : 4\n",
            "Train on 90000 samples, validate on 10000 samples\n",
            "Epoch 1/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5769 - policy_loss: 3.2693 - value_loss: 0.2216 - policy_acc: 0.2671 - value_acc: 0.6353\n",
            "Epoch 00001: val_loss did not improve from 3.40709\n",
            "90000/90000 [==============================] - 26s 293us/sample - loss: 3.5772 - policy_loss: 3.2698 - value_loss: 0.2216 - policy_acc: 0.2671 - value_acc: 0.6352 - val_loss: 3.4382 - val_policy_loss: 3.1360 - val_value_loss: 0.2184 - val_policy_acc: 0.2789 - val_value_acc: 0.6303\n",
            "Epoch 2/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5565 - policy_loss: 3.2489 - value_loss: 0.2216 - policy_acc: 0.2697 - value_acc: 0.6351\n",
            "Epoch 00002: val_loss did not improve from 3.40709\n",
            "90000/90000 [==============================] - 26s 293us/sample - loss: 3.5564 - policy_loss: 3.2489 - value_loss: 0.2216 - policy_acc: 0.2696 - value_acc: 0.6352 - val_loss: 3.4257 - val_policy_loss: 3.1276 - val_value_loss: 0.2187 - val_policy_acc: 0.2790 - val_value_acc: 0.6364\n",
            "Epoch 3/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5433 - policy_loss: 3.2362 - value_loss: 0.2213 - policy_acc: 0.2721 - value_acc: 0.6358\n",
            "Epoch 00003: val_loss did not improve from 3.40709\n",
            "90000/90000 [==============================] - 26s 293us/sample - loss: 3.5431 - policy_loss: 3.2360 - value_loss: 0.2213 - policy_acc: 0.2720 - value_acc: 0.6359 - val_loss: 3.4291 - val_policy_loss: 3.1082 - val_value_loss: 0.2195 - val_policy_acc: 0.2779 - val_value_acc: 0.6335\n",
            "end of iteration : 5\n",
            "Train on 90000 samples, validate on 10000 samples\n",
            "Epoch 1/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5913 - policy_loss: 3.2844 - value_loss: 0.2212 - policy_acc: 0.2624 - value_acc: 0.6358\n",
            "Epoch 00001: val_loss did not improve from 3.40709\n",
            "90000/90000 [==============================] - 26s 294us/sample - loss: 3.5915 - policy_loss: 3.2846 - value_loss: 0.2212 - policy_acc: 0.2623 - value_acc: 0.6358 - val_loss: 3.4387 - val_policy_loss: 3.1338 - val_value_loss: 0.2175 - val_policy_acc: 0.2858 - val_value_acc: 0.6422\n",
            "Epoch 2/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5749 - policy_loss: 3.2679 - value_loss: 0.2213 - policy_acc: 0.2665 - value_acc: 0.6363\n",
            "Epoch 00002: val_loss did not improve from 3.40709\n",
            "90000/90000 [==============================] - 26s 293us/sample - loss: 3.5746 - policy_loss: 3.2675 - value_loss: 0.2213 - policy_acc: 0.2665 - value_acc: 0.6363 - val_loss: 3.4371 - val_policy_loss: 3.1488 - val_value_loss: 0.2167 - val_policy_acc: 0.2892 - val_value_acc: 0.6481\n",
            "Epoch 3/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5602 - policy_loss: 3.2536 - value_loss: 0.2210 - policy_acc: 0.2685 - value_acc: 0.6367\n",
            "Epoch 00003: val_loss did not improve from 3.40709\n",
            "90000/90000 [==============================] - 26s 294us/sample - loss: 3.5600 - policy_loss: 3.2532 - value_loss: 0.2210 - policy_acc: 0.2685 - value_acc: 0.6367 - val_loss: 3.4517 - val_policy_loss: 3.1301 - val_value_loss: 0.2171 - val_policy_acc: 0.2862 - val_value_acc: 0.6466\n",
            "end of iteration : 6\n",
            "Train on 90000 samples, validate on 10000 samples\n",
            "Epoch 1/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5806 - policy_loss: 3.2739 - value_loss: 0.2211 - policy_acc: 0.2657 - value_acc: 0.6365\n",
            "Epoch 00001: val_loss improved from 3.40709 to 3.38665, saving model to best_val_loss_iter_train.h5\n",
            "90000/90000 [==============================] - 27s 295us/sample - loss: 3.5806 - policy_loss: 3.2739 - value_loss: 0.2212 - policy_acc: 0.2657 - value_acc: 0.6364 - val_loss: 3.3867 - val_policy_loss: 3.0966 - val_value_loss: 0.2131 - val_policy_acc: 0.2847 - val_value_acc: 0.6565\n",
            "Epoch 2/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5596 - policy_loss: 3.2531 - value_loss: 0.2210 - policy_acc: 0.2689 - value_acc: 0.6366\n",
            "Epoch 00002: val_loss did not improve from 3.38665\n",
            "90000/90000 [==============================] - 26s 294us/sample - loss: 3.5596 - policy_loss: 3.2532 - value_loss: 0.2210 - policy_acc: 0.2689 - value_acc: 0.6367 - val_loss: 3.4044 - val_policy_loss: 3.1131 - val_value_loss: 0.2136 - val_policy_acc: 0.2810 - val_value_acc: 0.6555\n",
            "Epoch 3/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5524 - policy_loss: 3.2458 - value_loss: 0.2212 - policy_acc: 0.2686 - value_acc: 0.6367\n",
            "Epoch 00003: val_loss did not improve from 3.38665\n",
            "90000/90000 [==============================] - 26s 293us/sample - loss: 3.5523 - policy_loss: 3.2455 - value_loss: 0.2212 - policy_acc: 0.2686 - value_acc: 0.6367 - val_loss: 3.4062 - val_policy_loss: 3.1045 - val_value_loss: 0.2138 - val_policy_acc: 0.2852 - val_value_acc: 0.6536\n",
            "end of iteration : 7\n",
            "Train on 90000 samples, validate on 10000 samples\n",
            "Epoch 1/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5811 - policy_loss: 3.2740 - value_loss: 0.2217 - policy_acc: 0.2671 - value_acc: 0.6346\n",
            "Epoch 00001: val_loss did not improve from 3.38665\n",
            "90000/90000 [==============================] - 26s 293us/sample - loss: 3.5816 - policy_loss: 3.2748 - value_loss: 0.2217 - policy_acc: 0.2670 - value_acc: 0.6345 - val_loss: 3.3934 - val_policy_loss: 3.0909 - val_value_loss: 0.2173 - val_policy_acc: 0.2866 - val_value_acc: 0.6422\n",
            "Epoch 2/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5589 - policy_loss: 3.2519 - value_loss: 0.2216 - policy_acc: 0.2683 - value_acc: 0.6357\n",
            "Epoch 00002: val_loss did not improve from 3.38665\n",
            "90000/90000 [==============================] - 26s 293us/sample - loss: 3.5589 - policy_loss: 3.2520 - value_loss: 0.2216 - policy_acc: 0.2684 - value_acc: 0.6357 - val_loss: 3.4039 - val_policy_loss: 3.0984 - val_value_loss: 0.2182 - val_policy_acc: 0.2863 - val_value_acc: 0.6433\n",
            "Epoch 3/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5441 - policy_loss: 3.2373 - value_loss: 0.2215 - policy_acc: 0.2709 - value_acc: 0.6349\n",
            "Epoch 00003: val_loss did not improve from 3.38665\n",
            "90000/90000 [==============================] - 26s 293us/sample - loss: 3.5442 - policy_loss: 3.2374 - value_loss: 0.2216 - policy_acc: 0.2709 - value_acc: 0.6347 - val_loss: 3.3918 - val_policy_loss: 3.0986 - val_value_loss: 0.2187 - val_policy_acc: 0.2897 - val_value_acc: 0.6427\n",
            "end of iteration : 8\n",
            "Train on 90000 samples, validate on 10000 samples\n",
            "Epoch 1/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5764 - policy_loss: 3.2701 - value_loss: 0.2211 - policy_acc: 0.2677 - value_acc: 0.6374\n",
            "Epoch 00001: val_loss did not improve from 3.38665\n",
            "90000/90000 [==============================] - 26s 293us/sample - loss: 3.5767 - policy_loss: 3.2707 - value_loss: 0.2211 - policy_acc: 0.2677 - value_acc: 0.6375 - val_loss: 3.4129 - val_policy_loss: 3.1213 - val_value_loss: 0.2157 - val_policy_acc: 0.2888 - val_value_acc: 0.6466\n",
            "Epoch 2/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5584 - policy_loss: 3.2519 - value_loss: 0.2213 - policy_acc: 0.2696 - value_acc: 0.6377\n",
            "Epoch 00002: val_loss did not improve from 3.38665\n",
            "90000/90000 [==============================] - 26s 294us/sample - loss: 3.5582 - policy_loss: 3.2516 - value_loss: 0.2213 - policy_acc: 0.2696 - value_acc: 0.6376 - val_loss: 3.4184 - val_policy_loss: 3.1106 - val_value_loss: 0.2175 - val_policy_acc: 0.2881 - val_value_acc: 0.6455\n",
            "Epoch 3/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5354 - policy_loss: 3.2292 - value_loss: 0.2210 - policy_acc: 0.2725 - value_acc: 0.6375\n",
            "Epoch 00003: val_loss did not improve from 3.38665\n",
            "90000/90000 [==============================] - 26s 293us/sample - loss: 3.5352 - policy_loss: 3.2289 - value_loss: 0.2210 - policy_acc: 0.2725 - value_acc: 0.6376 - val_loss: 3.4041 - val_policy_loss: 3.1019 - val_value_loss: 0.2155 - val_policy_acc: 0.2907 - val_value_acc: 0.6469\n",
            "end of iteration : 9\n",
            "Train on 90000 samples, validate on 10000 samples\n",
            "Epoch 1/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5800 - policy_loss: 3.2740 - value_loss: 0.2209 - policy_acc: 0.2649 - value_acc: 0.6378\n",
            "Epoch 00001: val_loss did not improve from 3.38665\n",
            "90000/90000 [==============================] - 26s 293us/sample - loss: 3.5800 - policy_loss: 3.2739 - value_loss: 0.2209 - policy_acc: 0.2650 - value_acc: 0.6378 - val_loss: 3.4111 - val_policy_loss: 3.1037 - val_value_loss: 0.2143 - val_policy_acc: 0.2844 - val_value_acc: 0.6512\n",
            "Epoch 2/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5588 - policy_loss: 3.2528 - value_loss: 0.2209 - policy_acc: 0.2694 - value_acc: 0.6378\n",
            "Epoch 00002: val_loss did not improve from 3.38665\n",
            "90000/90000 [==============================] - 26s 293us/sample - loss: 3.5590 - policy_loss: 3.2532 - value_loss: 0.2209 - policy_acc: 0.2694 - value_acc: 0.6379 - val_loss: 3.4449 - val_policy_loss: 3.1432 - val_value_loss: 0.2160 - val_policy_acc: 0.2841 - val_value_acc: 0.6516\n",
            "Epoch 3/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5435 - policy_loss: 3.2374 - value_loss: 0.2212 - policy_acc: 0.2716 - value_acc: 0.6372\n",
            "Epoch 00003: val_loss did not improve from 3.38665\n",
            "90000/90000 [==============================] - 26s 294us/sample - loss: 3.5440 - policy_loss: 3.2383 - value_loss: 0.2211 - policy_acc: 0.2715 - value_acc: 0.6372 - val_loss: 3.4501 - val_policy_loss: 3.1542 - val_value_loss: 0.2167 - val_policy_acc: 0.2819 - val_value_acc: 0.6504\n",
            "end of iteration : 10\n",
            "Train on 90000 samples, validate on 10000 samples\n",
            "Epoch 1/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5798 - policy_loss: 3.2740 - value_loss: 0.2208 - policy_acc: 0.2639 - value_acc: 0.6368\n",
            "Epoch 00001: val_loss did not improve from 3.38665\n",
            "90000/90000 [==============================] - 26s 293us/sample - loss: 3.5798 - policy_loss: 3.2740 - value_loss: 0.2208 - policy_acc: 0.2639 - value_acc: 0.6367 - val_loss: 3.4414 - val_policy_loss: 3.1260 - val_value_loss: 0.2185 - val_policy_acc: 0.2854 - val_value_acc: 0.6362\n",
            "Epoch 2/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5615 - policy_loss: 3.2558 - value_loss: 0.2208 - policy_acc: 0.2696 - value_acc: 0.6374\n",
            "Epoch 00002: val_loss did not improve from 3.38665\n",
            "90000/90000 [==============================] - 26s 293us/sample - loss: 3.5615 - policy_loss: 3.2558 - value_loss: 0.2208 - policy_acc: 0.2696 - value_acc: 0.6374 - val_loss: 3.3986 - val_policy_loss: 3.0750 - val_value_loss: 0.2178 - val_policy_acc: 0.2930 - val_value_acc: 0.6395\n",
            "Epoch 3/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5467 - policy_loss: 3.2411 - value_loss: 0.2207 - policy_acc: 0.2705 - value_acc: 0.6373\n",
            "Epoch 00003: val_loss did not improve from 3.38665\n",
            "90000/90000 [==============================] - 26s 293us/sample - loss: 3.5466 - policy_loss: 3.2408 - value_loss: 0.2207 - policy_acc: 0.2706 - value_acc: 0.6373 - val_loss: 3.4030 - val_policy_loss: 3.1148 - val_value_loss: 0.2164 - val_policy_acc: 0.2885 - val_value_acc: 0.6396\n",
            "end of iteration : 11\n",
            "Train on 90000 samples, validate on 10000 samples\n",
            "Epoch 1/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5793 - policy_loss: 3.2733 - value_loss: 0.2211 - policy_acc: 0.2655 - value_acc: 0.6365\n",
            "Epoch 00001: val_loss did not improve from 3.38665\n",
            "90000/90000 [==============================] - 26s 293us/sample - loss: 3.5788 - policy_loss: 3.2725 - value_loss: 0.2210 - policy_acc: 0.2654 - value_acc: 0.6365 - val_loss: 3.4038 - val_policy_loss: 3.1235 - val_value_loss: 0.2164 - val_policy_acc: 0.2831 - val_value_acc: 0.6451\n",
            "Epoch 2/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5528 - policy_loss: 3.2470 - value_loss: 0.2209 - policy_acc: 0.2681 - value_acc: 0.6365\n",
            "Epoch 00002: val_loss did not improve from 3.38665\n",
            "90000/90000 [==============================] - 26s 292us/sample - loss: 3.5527 - policy_loss: 3.2469 - value_loss: 0.2209 - policy_acc: 0.2681 - value_acc: 0.6365 - val_loss: 3.4381 - val_policy_loss: 3.1144 - val_value_loss: 0.2194 - val_policy_acc: 0.2864 - val_value_acc: 0.6425\n",
            "Epoch 3/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5410 - policy_loss: 3.2354 - value_loss: 0.2207 - policy_acc: 0.2719 - value_acc: 0.6374\n",
            "Epoch 00003: val_loss did not improve from 3.38665\n",
            "90000/90000 [==============================] - 26s 293us/sample - loss: 3.5408 - policy_loss: 3.2350 - value_loss: 0.2207 - policy_acc: 0.2719 - value_acc: 0.6374 - val_loss: 3.4356 - val_policy_loss: 3.1319 - val_value_loss: 0.2185 - val_policy_acc: 0.2845 - val_value_acc: 0.6445\n",
            "end of iteration : 12\n",
            "Train on 90000 samples, validate on 10000 samples\n",
            "Epoch 1/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5813 - policy_loss: 3.2751 - value_loss: 0.2213 - policy_acc: 0.2664 - value_acc: 0.6358\n",
            "Epoch 00001: val_loss improved from 3.38665 to 3.37181, saving model to best_val_loss_iter_train.h5\n",
            "90000/90000 [==============================] - 27s 295us/sample - loss: 3.5815 - policy_loss: 3.2755 - value_loss: 0.2213 - policy_acc: 0.2664 - value_acc: 0.6358 - val_loss: 3.3718 - val_policy_loss: 3.0751 - val_value_loss: 0.2168 - val_policy_acc: 0.2907 - val_value_acc: 0.6483\n",
            "Epoch 2/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5510 - policy_loss: 3.2449 - value_loss: 0.2212 - policy_acc: 0.2704 - value_acc: 0.6354\n",
            "Epoch 00002: val_loss did not improve from 3.37181\n",
            "90000/90000 [==============================] - 26s 293us/sample - loss: 3.5508 - policy_loss: 3.2445 - value_loss: 0.2212 - policy_acc: 0.2704 - value_acc: 0.6356 - val_loss: 3.3824 - val_policy_loss: 3.0638 - val_value_loss: 0.2164 - val_policy_acc: 0.2906 - val_value_acc: 0.6470\n",
            "Epoch 3/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5368 - policy_loss: 3.2309 - value_loss: 0.2211 - policy_acc: 0.2725 - value_acc: 0.6365\n",
            "Epoch 00003: val_loss did not improve from 3.37181\n",
            "90000/90000 [==============================] - 26s 293us/sample - loss: 3.5368 - policy_loss: 3.2308 - value_loss: 0.2211 - policy_acc: 0.2725 - value_acc: 0.6364 - val_loss: 3.3819 - val_policy_loss: 3.0881 - val_value_loss: 0.2165 - val_policy_acc: 0.2906 - val_value_acc: 0.6469\n",
            "end of iteration : 13\n",
            "Train on 90000 samples, validate on 10000 samples\n",
            "Epoch 1/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5711 - policy_loss: 3.2654 - value_loss: 0.2209 - policy_acc: 0.2669 - value_acc: 0.6375\n",
            "Epoch 00001: val_loss did not improve from 3.37181\n",
            "90000/90000 [==============================] - 26s 293us/sample - loss: 3.5714 - policy_loss: 3.2658 - value_loss: 0.2209 - policy_acc: 0.2669 - value_acc: 0.6376 - val_loss: 3.4011 - val_policy_loss: 3.1053 - val_value_loss: 0.2147 - val_policy_acc: 0.2884 - val_value_acc: 0.6531\n",
            "Epoch 2/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5541 - policy_loss: 3.2487 - value_loss: 0.2205 - policy_acc: 0.2693 - value_acc: 0.6385\n",
            "Epoch 00002: val_loss did not improve from 3.37181\n",
            "90000/90000 [==============================] - 26s 294us/sample - loss: 3.5542 - policy_loss: 3.2490 - value_loss: 0.2205 - policy_acc: 0.2693 - value_acc: 0.6385 - val_loss: 3.4155 - val_policy_loss: 3.1209 - val_value_loss: 0.2157 - val_policy_acc: 0.2862 - val_value_acc: 0.6533\n",
            "Epoch 3/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5414 - policy_loss: 3.2360 - value_loss: 0.2205 - policy_acc: 0.2705 - value_acc: 0.6380\n",
            "Epoch 00003: val_loss did not improve from 3.37181\n",
            "90000/90000 [==============================] - 26s 293us/sample - loss: 3.5415 - policy_loss: 3.2363 - value_loss: 0.2205 - policy_acc: 0.2704 - value_acc: 0.6380 - val_loss: 3.4335 - val_policy_loss: 3.1161 - val_value_loss: 0.2163 - val_policy_acc: 0.2792 - val_value_acc: 0.6503\n",
            "end of iteration : 14\n",
            "Train on 90000 samples, validate on 10000 samples\n",
            "Epoch 1/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5713 - policy_loss: 3.2647 - value_loss: 0.2217 - policy_acc: 0.2662 - value_acc: 0.6347\n",
            "Epoch 00001: val_loss did not improve from 3.37181\n",
            "90000/90000 [==============================] - 26s 293us/sample - loss: 3.5709 - policy_loss: 3.2640 - value_loss: 0.2218 - policy_acc: 0.2663 - value_acc: 0.6346 - val_loss: 3.3739 - val_policy_loss: 3.0573 - val_value_loss: 0.2180 - val_policy_acc: 0.2958 - val_value_acc: 0.6412\n",
            "Epoch 2/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5543 - policy_loss: 3.2476 - value_loss: 0.2218 - policy_acc: 0.2693 - value_acc: 0.6347\n",
            "Epoch 00002: val_loss did not improve from 3.37181\n",
            "90000/90000 [==============================] - 26s 293us/sample - loss: 3.5539 - policy_loss: 3.2470 - value_loss: 0.2219 - policy_acc: 0.2694 - value_acc: 0.6347 - val_loss: 3.3733 - val_policy_loss: 3.0682 - val_value_loss: 0.2182 - val_policy_acc: 0.2937 - val_value_acc: 0.6401\n",
            "Epoch 3/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5406 - policy_loss: 3.2341 - value_loss: 0.2217 - policy_acc: 0.2716 - value_acc: 0.6349\n",
            "Epoch 00003: val_loss did not improve from 3.37181\n",
            "90000/90000 [==============================] - 26s 293us/sample - loss: 3.5406 - policy_loss: 3.2341 - value_loss: 0.2217 - policy_acc: 0.2715 - value_acc: 0.6349 - val_loss: 3.3719 - val_policy_loss: 3.0597 - val_value_loss: 0.2182 - val_policy_acc: 0.2907 - val_value_acc: 0.6411\n",
            "end of iteration : 15\n",
            "Train on 90000 samples, validate on 10000 samples\n",
            "Epoch 1/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5704 - policy_loss: 3.2643 - value_loss: 0.2213 - policy_acc: 0.2661 - value_acc: 0.6339\n",
            "Epoch 00001: val_loss did not improve from 3.37181\n",
            "90000/90000 [==============================] - 26s 293us/sample - loss: 3.5703 - policy_loss: 3.2642 - value_loss: 0.2213 - policy_acc: 0.2661 - value_acc: 0.6340 - val_loss: 3.4083 - val_policy_loss: 3.1182 - val_value_loss: 0.2174 - val_policy_acc: 0.2850 - val_value_acc: 0.6440\n",
            "Epoch 2/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5530 - policy_loss: 3.2470 - value_loss: 0.2212 - policy_acc: 0.2688 - value_acc: 0.6348\n",
            "Epoch 00002: val_loss did not improve from 3.37181\n",
            "90000/90000 [==============================] - 26s 293us/sample - loss: 3.5528 - policy_loss: 3.2466 - value_loss: 0.2212 - policy_acc: 0.2688 - value_acc: 0.6348 - val_loss: 3.4223 - val_policy_loss: 3.1115 - val_value_loss: 0.2177 - val_policy_acc: 0.2842 - val_value_acc: 0.6444\n",
            "Epoch 3/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5363 - policy_loss: 3.2305 - value_loss: 0.2210 - policy_acc: 0.2733 - value_acc: 0.6355\n",
            "Epoch 00003: val_loss did not improve from 3.37181\n",
            "90000/90000 [==============================] - 26s 293us/sample - loss: 3.5360 - policy_loss: 3.2300 - value_loss: 0.2210 - policy_acc: 0.2734 - value_acc: 0.6354 - val_loss: 3.4070 - val_policy_loss: 3.1005 - val_value_loss: 0.2178 - val_policy_acc: 0.2823 - val_value_acc: 0.6451\n",
            "end of iteration : 16\n",
            "Train on 90000 samples, validate on 10000 samples\n",
            "Epoch 1/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5630 - policy_loss: 3.2568 - value_loss: 0.2214 - policy_acc: 0.2681 - value_acc: 0.6363\n",
            "Epoch 00001: val_loss improved from 3.37181 to 3.34838, saving model to best_val_loss_iter_train.h5\n",
            "90000/90000 [==============================] - 26s 294us/sample - loss: 3.5635 - policy_loss: 3.2575 - value_loss: 0.2215 - policy_acc: 0.2680 - value_acc: 0.6362 - val_loss: 3.3484 - val_policy_loss: 3.0527 - val_value_loss: 0.2182 - val_policy_acc: 0.2909 - val_value_acc: 0.6419\n",
            "Epoch 2/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5431 - policy_loss: 3.2366 - value_loss: 0.2216 - policy_acc: 0.2713 - value_acc: 0.6361\n",
            "Epoch 00002: val_loss did not improve from 3.34838\n",
            "90000/90000 [==============================] - 26s 292us/sample - loss: 3.5430 - policy_loss: 3.2364 - value_loss: 0.2217 - policy_acc: 0.2714 - value_acc: 0.6361 - val_loss: 3.3821 - val_policy_loss: 3.0848 - val_value_loss: 0.2180 - val_policy_acc: 0.2908 - val_value_acc: 0.6412\n",
            "Epoch 3/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5293 - policy_loss: 3.2232 - value_loss: 0.2213 - policy_acc: 0.2744 - value_acc: 0.6364\n",
            "Epoch 00003: val_loss did not improve from 3.34838\n",
            "90000/90000 [==============================] - 26s 293us/sample - loss: 3.5292 - policy_loss: 3.2230 - value_loss: 0.2213 - policy_acc: 0.2744 - value_acc: 0.6364 - val_loss: 3.3933 - val_policy_loss: 3.0648 - val_value_loss: 0.2186 - val_policy_acc: 0.2868 - val_value_acc: 0.6404\n",
            "end of iteration : 17\n",
            "Train on 90000 samples, validate on 10000 samples\n",
            "Epoch 1/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5599 - policy_loss: 3.2537 - value_loss: 0.2213 - policy_acc: 0.2682 - value_acc: 0.6360\n",
            "Epoch 00001: val_loss did not improve from 3.34838\n",
            "90000/90000 [==============================] - 26s 293us/sample - loss: 3.5597 - policy_loss: 3.2534 - value_loss: 0.2213 - policy_acc: 0.2682 - value_acc: 0.6359 - val_loss: 3.4262 - val_policy_loss: 3.1301 - val_value_loss: 0.2168 - val_policy_acc: 0.2889 - val_value_acc: 0.6452\n",
            "Epoch 2/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5393 - policy_loss: 3.2333 - value_loss: 0.2212 - policy_acc: 0.2708 - value_acc: 0.6361\n",
            "Epoch 00002: val_loss did not improve from 3.34838\n",
            "90000/90000 [==============================] - 26s 292us/sample - loss: 3.5393 - policy_loss: 3.2333 - value_loss: 0.2212 - policy_acc: 0.2708 - value_acc: 0.6361 - val_loss: 3.4071 - val_policy_loss: 3.1013 - val_value_loss: 0.2156 - val_policy_acc: 0.2903 - val_value_acc: 0.6450\n",
            "Epoch 3/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5237 - policy_loss: 3.2175 - value_loss: 0.2214 - policy_acc: 0.2731 - value_acc: 0.6359\n",
            "Epoch 00003: val_loss did not improve from 3.34838\n",
            "90000/90000 [==============================] - 26s 293us/sample - loss: 3.5237 - policy_loss: 3.2174 - value_loss: 0.2214 - policy_acc: 0.2731 - value_acc: 0.6359 - val_loss: 3.3946 - val_policy_loss: 3.0736 - val_value_loss: 0.2155 - val_policy_acc: 0.2895 - val_value_acc: 0.6450\n",
            "end of iteration : 18\n",
            "Train on 90000 samples, validate on 10000 samples\n",
            "Epoch 1/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5677 - policy_loss: 3.2624 - value_loss: 0.2204 - policy_acc: 0.2664 - value_acc: 0.6411\n",
            "Epoch 00001: val_loss did not improve from 3.34838\n",
            "90000/90000 [==============================] - 26s 293us/sample - loss: 3.5675 - policy_loss: 3.2620 - value_loss: 0.2204 - policy_acc: 0.2665 - value_acc: 0.6411 - val_loss: 3.3706 - val_policy_loss: 3.0676 - val_value_loss: 0.2164 - val_policy_acc: 0.2890 - val_value_acc: 0.6471\n",
            "Epoch 2/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5492 - policy_loss: 3.2442 - value_loss: 0.2201 - policy_acc: 0.2690 - value_acc: 0.6410\n",
            "Epoch 00002: val_loss did not improve from 3.34838\n",
            "90000/90000 [==============================] - 26s 293us/sample - loss: 3.5493 - policy_loss: 3.2444 - value_loss: 0.2201 - policy_acc: 0.2691 - value_acc: 0.6410 - val_loss: 3.3792 - val_policy_loss: 3.0544 - val_value_loss: 0.2155 - val_policy_acc: 0.2880 - val_value_acc: 0.6468\n",
            "Epoch 3/3\n",
            "89856/90000 [============================>.] - ETA: 0s - loss: 3.5321 - policy_loss: 3.2267 - value_loss: 0.2205 - policy_acc: 0.2704 - value_acc: 0.6406\n",
            "Epoch 00003: val_loss did not improve from 3.34838\n",
            "90000/90000 [==============================] - 27s 295us/sample - loss: 3.5325 - policy_loss: 3.2274 - value_loss: 0.2205 - policy_acc: 0.2705 - value_acc: 0.6406 - val_loss: 3.3760 - val_policy_loss: 3.0591 - val_value_loss: 0.2165 - val_policy_acc: 0.2897 - val_value_acc: 0.6475\n",
            "end of iteration : 19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NViWaRcmP0Pl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save weights re-trained model\n",
        "from google.colab import files\n",
        "files.download('best_val_loss_iter_train.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0TrVhESkOFm",
        "colab_type": "code",
        "outputId": "bfca8b5f-d593-4f76-bfb8-5955546a406c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "# Load pre-trained model result from iterative training\n",
        "from tensorflow.keras.models import load_model\n",
        "new_model = load_model('best_val_loss_iter_train.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-231040d6b68d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnew_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_val_loss_iter_train.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0mloader_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m     81\u001b[0m                   (export_dir,\n\u001b[1;32m     82\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                    constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: best_val_loss_iter_train.h5/{saved_model.pbtxt|saved_model.pb}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1TBGR88kRfg",
        "colab_type": "code",
        "outputId": "25cca117-7e1e-486e-a18e-dd1efdbf5b8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        }
      },
      "source": [
        "new_model.evaluate(input_data, {'policy': policy, 'value': value})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 93824/300000 [========>.....................] - ETA: 44s - loss: 3.2795 - policy_loss: 2.9736 - value_loss: 0.2170 - policy_acc: 0.3090 - value_acc: 0.6435"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-5c5652a93a93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'policy'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'value'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    830\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m   def predict(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, model, x, y, batch_size, verbose, sample_weight, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m         callbacks=callbacks)\n\u001b[0m\u001b[1;32m    705\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m   def predict(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    375\u001b[0m               \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m               \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m             raise TypeError('TypeError while preparing batch. '\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    527\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m     return [\n\u001b[1;32m    531\u001b[0m         \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    527\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m     return [\n\u001b[1;32m    531\u001b[0m         \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DHwOas5A31h",
        "colab_type": "text"
      },
      "source": [
        "# Train last dense layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mfcr4lGveqmI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load pre-trained model\n",
        "from tensorflow.keras.models import load_model\n",
        "new_model = load_model('best_val_loss_last_layer.h5')\n",
        "# new_model = load_model('./drive/My Drive/DLGO/best_val_loss.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELMl7R8B_6Cj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set layers to be trained\n",
        "my_layers = new_model.layers\n",
        "for layer in new_model.layers:\n",
        "    layer.trainable = False\n",
        "new_model.layers[60].trainable = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XT9m9CxMAXfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Recompile model before retraining\n",
        "# new_model.compile(optimizer=SGD(lr=0.001, momentum=0.9),\n",
        "#                       loss={\n",
        "#                           'value': 'mse',\n",
        "#                           'policy': 'categorical_crossentropy'\n",
        "#                       },\n",
        "#                       loss_weights={\n",
        "#                           'value': 1,\n",
        "#                           'policy': 1\n",
        "#                       },\n",
        "#                       metrics=['accuracy'])\n",
        "\n",
        "# Recompile model before retraining\n",
        "new_model.compile(optimizer=SGD(learning_rate=0.01),\n",
        "                      loss={\n",
        "                          'value': 'mse',\n",
        "                          'policy': 'categorical_crossentropy'\n",
        "                      },\n",
        "                      loss_weights={\n",
        "                          'value': 1,\n",
        "                          'policy': 1\n",
        "                      },\n",
        "                      metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXHyXCxKBGeC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ValCheckpoint = ModelCheckpoint('best_val_loss_last_layer.h5',\n",
        "                        monitor='val_loss',\n",
        "                        verbose=1,\n",
        "                        save_best_only=True,\n",
        "                        mode='auto',\n",
        "                        period=1)\n",
        "\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', \n",
        "                   patience=6,\n",
        "                   verbose=1,\n",
        "                   mode='min', \n",
        "                   restore_best_weights=True)\n",
        "\n",
        "\n",
        "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
        "#                               factor=0.5,\n",
        "#                               patience=3,\n",
        "#                               verbose=1,\n",
        "#                               mode='min',  \n",
        "#                               min_lr=0.00001)\n",
        "\n",
        "for i in range(10):\n",
        "    input_data_iter, policy_iter, value_iter = generateData(N=100000)\n",
        "\n",
        "    es = EarlyStopping(monitor='val_loss', \n",
        "                    patience=6,\n",
        "                    verbose=1,\n",
        "                    mode='min', \n",
        "                    restore_best_weights=True)\n",
        "\n",
        "    new_model.fit(input_data_iter, \n",
        "                {'policy': policy_iter,\n",
        "                'value': value_iter},\n",
        "                epochs=15,\n",
        "                verbose=1,\n",
        "                validation_split=0.1,\n",
        "                batch_size=256,\n",
        "                callbacks=[ValCheckpoint, es])\n",
        "    \n",
        "    del input_data_iter\n",
        "    del policy_iter\n",
        "    del value_iter\n",
        "    print('end of iteration :', i)\n",
        "    # new_model.evaluate(input_data, {'policy': policy, 'value': value})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zu52Fz1PK7BJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save weights re-trained model\n",
        "from google.colab import files\n",
        "files.download('best_val_loss_last_layer.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLKJ9tlOBfYv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load pre-trained model\n",
        "from tensorflow.keras.models import load_model\n",
        "new_model = load_model('best_val_loss_last_layer.h5')\n",
        "# new_model = load_model('./drive/My Drive/DLGO/best_val_loss.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9uKV6r1FXZc",
        "colab_type": "code",
        "outputId": "3c9eba7a-bb06-4055-b127-fa5b1012491d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Save weights re-trained model\n",
        "new_model.evaluate(input_data, {'policy': policy, 'value': value})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "300000/300000 [==============================] - 64s 212us/sample - loss: 3.2707 - policy_loss: 2.9628 - value_loss: 0.2168 - policy_acc: 0.3104 - value_acc: 0.6441\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3.270696885668437, 2.9627993, 0.21683921, 0.31041333, 0.64411336]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    }
  ]
}